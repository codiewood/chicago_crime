---
title: "Using Regression and Classification Techniques on Chicago Crime Dataset"
author: "Rahil Morjaria, Codie Wood, Rachel Wood"
output: pdf_document
date: "January 2023"
output:
  pdf_document: default
  html_document: default
date: "2023-01-09"
---



```{r}
devtools::install_github("codiewood/chicago_crime/chicri") 

```

```{r, message = FALSE}
library(tidyverse)
theme_set(theme_minimal())
cbpal <- c("#000000", "#E69F00", "#56B4E9", "#009E73","#F0E442", "#0072B2", "#D55E00", "#CC79A7")
library(chicri)
```
## Computational Techniques
### Package Building
Packages are a key part of producing reproducible R code. They include R functions which can be reused, the documentation that describes how to use them, and sometimes sample data. In the construction of our `chicri` package, we mainly used the `devtools` and `usethis` packages. We followed the guidelines and usage principles detailed in
[R Packages (Second Edition)]\url{https://r-pkgs.org/}
BEEP bibliography? BEEP, in order to ensure our package was consistent with widely used conventions.

### Documentation

When documenting the various functions in the package, we used the `roxygen2` package. This package automatically creates `.Rd` documentation files for functions from comments added as part of the function definitions. Using this framework allows the code and documentation to coexist and be updated alongside eachother, ensuring consistency.

### Application Programming Interface (API) Querying
 
The Chicago Crime dataset stored online is regularly updated on a daily basis. It has millions of rows of data, and each row contains a total of 22 features which have been recorded. As such, the full data is incredibly large and so costly to store. As such, we chose not to include data files in the package. Instead, we used the `RSocrata` package to interact with the online data portal and pull the data. The function `load_crimes_API()` allows the user to directly download the dataset into R, via the function `read.socrata()`, and also to specify a particular year from which the data should be loaded.

## Testing, GitHub and Continuous Integration
For tests, we used the `testthat` package. This creates a new directory `./tests/testthat/` to be populated with unit tests for the package functions. Although the tests can be run alone, generally, we made use of the `usethis::check()` command. This relies on the function `rcmdcheck::rcmdcheck()`, which not only runs the tests, but also carries out other checks for the package build. These include:

## Package Description


- Checking the package installs correctly,

- Checking for missing documentation,

- Checking for undefined variables,

- Checking for missing dependencies.

In order to ensure that tests and checks were run on a regular basis, continuous integration was set up using Github Actions. The status of this action can be seen in our package description markdown file. For each pull request and push to the main branch, this checked our package on multiple operating systems (Windows, Ubuntu and MacOS). This helped to ensure that when writing our code, we did not introduce any changes which broke functionality.

### Parallel Programming

BEEP rahil did this so should probs talk about it BEEP


# Exploratory Data Analysis
Our data can be loaded using the `load_crimes_API()` function from the `chicri` package:
```{r, eval=FALSE}
df <- load_crimes_API(year = NULL) %>% process_data()
```

```{r, include = FALSE}
library(tidyverse)
df<- read_csv("~/Desktop/project/data_API.csv", show_col_types = FALSE)
df <- df %>% long_variables()
```

## Variables to Keep




# Predicting Crimes Spatially and Temporally 
For this task we implement a general additive model (GAM) using the `gam()` function from the `mgcv` package.


## Weekly Predictions

We use the `count_cases()` function to obtain weekly and monthly counts
```{r}
library(tidyverse)
library(lubridate)

week_count <- df %>% count_cases(date_level = "week")

ggplot(data =week_count) +
  geom_point(aes(x= `Week Start`, y = Count), color = "steelblue3", alpha = 0.5)
```
We can see that there is a drop in early 2020 (likely due to the COVID pandemic), and so we only consider data from 2019 or earlier when fitting our model. We then split the data into testing and training sets:

```{r, warning=FALSE}
library(mgcv)

week_count <- week_count %>% filter(Year <= 2019, Year >= 2015)
week_train <- week_count %>% filter(Year < 2018) 
week_test <- week_count %>% filter(Year >= 2018)

model_week <- gam(Count ~ s(Week, bs = "cc") + Year, data = week_train, family = "poisson")

week_train <- week_train %>% cbind("Predicted" = model_week$fitted.values, "Data" = "Train")

week_test <- week_test %>% cbind("Predicted" = as.numeric(predict(model_week, week_test, type = "response")), "Data" = "Test")


week_dat <- rbind(week_train, week_test)


p <-ggplot(data = week_dat) +
  geom_point(aes(x = `Week Start` , y = Count), color = cbpal[3], alpha =0.5) +
  geom_line(aes(x = `Week Start`, y = Predicted, color = Data), size = 1) +
  scale_colour_manual(values= cbpal[c(7,6)])
p
```

## Weekly Predictions in Space

We can now consider how the crime rates vary within Chicago using the community bounds. The shapefiles for these are available on the Chicago Data Portal, and the `chicri` package contains a function for loading them. 

To include them in our `gam` model however, we need to use a Markov Random Field smooth. This creates an adjacency matrix for the community areas (based on whether any two areas share a vertex).
```{r}
library(RSocrata)
library(sf)
community_map <- get_map() %>%
  mutate(Area = as.factor(Area))

community_list <- setNames(as.list(community_map$the_geom), as.character(community_map$Area))

n <- nrow(community_map)
for (i in 1:n){
  object <- community_list[[i]] 
  #class(object)
  community_list[[i]] <- unlist(st_coordinates(object), recursive =TRUE)
}
names(community_list) <- as.character(community_map$Area)
xt <- list(polys = community_list)
```


As before, we obtain the count data, this time with the community area included, and obtain testing and training data set
```{r}
spatial_count <- df %>% count_cases(location_level = "community_area") %>%
  mutate(CommunityArea = as.factor(`Community Area`)) %>%
  select(- `Community Area`)



spatial_count <- spatial_count %>% filter(Year <= 2019, Year >= 2015)
spatial_train <- spatial_count %>% filter(Year < 2018) 
spatial_test <- spatial_count %>% filter(Year >= 2018)


spatial_model <- gam(Count ~ s(Month, bs = "cc") + Year + s(CommunityArea, bs = "mrf", xt = xt), data = spatial_train, family = "poisson")

summary(spatial_model)

```

### Incorporating Socio-Economic Indicators
The Chicago Data Portal also contains a dataset with three variables for each community area:

- `percent_households_below_poverty`: Percentage of households living the federal poverty line
- `per_capita_income_`: This is an estimation, calculated by aggregating incomes and dividing by the total population
- `hardship_index`: A score from 1-100 (a higher score indicates a greater level of hardship), incorporating 6 socio-economic indicators.



```{r}
library(corrplot)
socio_ind <- get_indicators()


socio_ind %>% select(-`Community Area`) %>% cor() %>% corrplot(method = "square", type = "lower", tl.pos = "l", tl.col = "black")
```

We can see that all of these values are highly correlated, and so we only need to include one in an analysis. For this we choose the hardship index as it is made up partly of the other two indicators. We now fit the model with the same method as before:

```{r}
spatial_count <- df %>% count_cases(location_level = "community_area") %>%
  rename("CommunityArea" = "Community Area") %>%
  mutate(CommunityArea = as.factor(CommunityArea))

spatial_count <- spatial_count %>%
  filter(Year >= 2007, Year <=2011)
socio_data <- spatial_count %>% left_join(socio_ind, by = c("CommunityArea" = "Community Area")) %>%
  select(-c(`Poverty Rate`)) %>%
  rename(HardshipIndex = `Hardship Index`)

socio_train <- socio_data %>% filter(Year < 2019) 
socio_test <- socio_data %>% filter(Year >= 2019)

socio_model <- gam(Count ~ s(Month, bs = "cc") + Year + s(CommunityArea, bs = "mrf", xt = xt) + HardshipIndex, data = socio_data, family = "poisson")

summary(socio_model)
```


