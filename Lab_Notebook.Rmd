---
title: "Using Regression and Classification Techniques on Chicago Crime Dataset"
author: "Rahil Morjaria, Codie Wood, Rachel Wood"
output: 
  pdf_document:
    keep_tex: yes
date: "January 2023"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      dev = 'pdf',
                      pdf.options(encoding = "ISOLatin9.enc"))
```


```{r, eval=FALSE}
devtools::install_github("codiewood/chicago_crime/chicri") 

```

```{r, message = FALSE}
library(tidyverse)
theme_set(theme_minimal())
cbpal <- c("#000000", "#E69F00", "#56B4E9", "#009E73","#F0E442", "#0072B2", "#D55E00", "#CC79A7")
library(chicri)
```
## Computational Techniques
### Package Building
Packages are a key part of producing reproducible R code. They include R functions which can be reused, the documentation that describes how to use them, and sometimes sample data. In the construction of our `chicri` package, we mainly used the `devtools` and `usethis` packages. We followed the guidelines and usage principles detailed in
[R Packages (Second Edition)]\url{https://r-pkgs.org/}
BEEP bibliography? BEEP, in order to ensure our package was consistent with widely used conventions.

### Documentation

When documenting the various functions in the package, we used the `roxygen2` package. This package automatically creates `.Rd` documentation files for functions from comments added as part of the function definitions. Using this framework allows the code and documentation to coexist and be updated alongside eachother, ensuring consistency.

### Application Programming Interface (API) Querying
 
The Chicago Crime dataset stored online is regularly updated on a daily basis. It has millions of rows of data, and each row contains a total of 22 features which have been recorded. As such, the full data is incredibly large and so costly to store. As such, we chose not to include data files in the package. Instead, we used the `RSocrata` package to interact with the online data portal and pull the data. The function `load_crimes_API()` allows the user to directly download the dataset into R, via the function `read.socrata()`, and also to specify a particular year from which the data should be loaded.

## Testing, GitHub and Continuous Integration
For tests, we used the `testthat` package. This creates a new directory `./tests/testthat/` to be populated with unit tests for the package functions. Although the tests can be run alone, generally, we made use of the `usethis::check()` command. This relies on the function `rcmdcheck::rcmdcheck()`, which not only runs the tests, but also carries out other checks for the package build. These include:

## Package Description


- Checking the package installs correctly,

- Checking for missing documentation,

- Checking for undefined variables,

- Checking for missing dependencies.

In order to ensure that tests and checks were run on a regular basis, continuous integration was set up using Github Actions. The status of this action can be seen in our package description markdown file. For each pull request and push to the main branch, this checked our package on multiple operating systems (Windows, Ubuntu and MacOS). This helped to ensure that when writing our code, we did not introduce any changes which broke functionality.

### Parallel Programming

We used parallel programming, which allows code to be run across multiple cores, in order to speed up code with long run times. We use the package `doParallel` in order to implement this. In particular, we use parallel programming in the cross-validation for our binary classifiers, allowing faster assessment of model performance, and more models to be assessed in a shorter time frame.

# Exploratory Data Analysis
Our data can be loaded using the `load_crimes_API()` function from the `chicri` package:
```{r, eval=FALSE}
df <- load_crimes_API(year = NULL) %>% process_data()
```

```{r, include = FALSE}
library(tidyverse)
df<- read_csv("~/Desktop/project/data_API.csv", show_col_types = FALSE)
df <- df %>% long_variables()
```

## Variables to Keep




# Predicting Crimes Spatially and Temporally 
For this task we implement a general additive model (GAM) using the `gam()` function from the `mgcv` package.


## Weekly Predictions

We use the `count_cases()` function to obtain weekly and monthly counts
```{r}
library(tidyverse)
library(lubridate)

week_count <- df %>% count_cases(date_level = "week")

ggplot(data =week_count) +
  geom_point(aes(x= `Week Start`, y = Count), color = "steelblue3", alpha = 0.5)
```
We can see that there is a drop in early 2020 (likely due to the COVID pandemic), and so we only consider data from 2019 or earlier when fitting our model. We then split the data into testing and training sets:

```{r, warning=FALSE}
library(mgcv)

week_count <- week_count %>% filter(Year <= 2019, Year >= 2015)
week_train <- week_count %>% filter(Year < 2018) 
week_test <- week_count %>% filter(Year >= 2018)

model_week <- gam(Count ~ s(Week, bs = "cc") + Year, data = week_train, family = "poisson")

week_train <- week_train %>% cbind("Predicted" = model_week$fitted.values, "Data" = "Train")

week_test <- week_test %>% cbind("Predicted" = as.numeric(predict(model_week, week_test, type = "response")), "Data" = "Test")


week_dat <- rbind(week_train, week_test)


p <-ggplot(data = week_dat) +
  geom_point(aes(x = `Week Start` , y = Count), color = cbpal[3], alpha =0.5) +
  geom_line(aes(x = `Week Start`, y = Predicted, color = Data), size = 1) +
  scale_colour_manual(values= cbpal[c(7,6)])
p
```


While this model looks reasonable, we also perform some model checking. The `draw()` function allows us to visualise the partial effects of the model:

```{r, warning = FALSE}
library(gratia)
library(patchwork)
library(mgcViz)
effects <- draw(model_week)

effects

gam.check(model_week)

check <- model_week %>% getViz() 

check(check,
      a.qq = list(a.cipoly = list(fill = "light blue")), 
      a.respoi = list(size = 0.5), 
      a.hist = list(bins = 10)
      )

```

## Weekly Predictions in Space

We can now consider how the crime rates vary within Chicago using the community bounds. The shapefiles for these are available on the Chicago Data Portal, and the `chicri` package contains a function for loading them. 

To include them in our `gam` model however, we need to use a Markov Random Field smooth. This creates an adjacency matrix for the community areas (based on whether any two areas share a vertex).
```{r}
library(RSocrata)
library(sf)
library(chicri)
community_map <- get_map() %>%
  mutate(Area = as.factor(Area))

community_list <- setNames(as.list(community_map$the_geom), as.character(community_map$Area))

n <- nrow(community_map)
for (i in 1:n){
  object <- community_list[[i]] 
  #class(object)
  community_list[[i]] <- unlist(st_coordinates(object), recursive =TRUE)
}
names(community_list) <- as.character(community_map$Area)
xt <- list(polys = community_list)
```


As before, we obtain the count data, this time with the community area included, and obtain testing and training data set
```{r}
spatial_count <- df %>% count_cases(location_level = "community_area") %>%
  mutate(CommunityArea = as.factor(`Community Area`)) %>%
  select(- `Community Area`)



spatial_count <- spatial_count %>% filter(Year <= 2019, Year >= 2015)
spatial_train <- spatial_count %>% filter(Year < 2018) 
spatial_test <- spatial_count %>% filter(Year >= 2018)


spatial_model <- gam(Count ~ s(Month, bs = "cc") + Year + s(CommunityArea, bs = "mrf", xt = xt), data = spatial_train, family = "poisson")

summary(spatial_model)

check_spatial <- spatial_model %>% getViz() 

check(check_spatial,
      a.qq = list(a.cipoly = list(fill = "light blue")), 
      a.respoi = list(size = 0.5), 
      a.hist = list(bins = 10)
      )
```




### Incorporating Socio-Economic Indicators
The Chicago Data Portal also contains a dataset with three variables for each community area:

- `percent_households_below_poverty`: Percentage of households living the federal poverty line
- `per_capita_income_`: This is an estimation, calculated by aggregating incomes and dividing by the total population
- `hardship_index`: A score from 1-100 (a higher score indicates a greater level of hardship), incorporating 6 socio-economic indicators.



```{r}
library(corrplot)
socio_ind <- get_indicators()


socio_ind %>% select(-`Community Area`) %>% cor() %>% corrplot(method = "square", type = "lower", tl.pos = "l", tl.col = "black")
```

We can see that all of these values are highly correlated, and so we only need to include one in an analysis. For this we choose the hardship index as it is made up partly of the other two indicators. We now fit the model with the same method as before:

```{r}
spatial_count <- df %>% count_cases(location_level = "community_area") %>%
  rename("CommunityArea" = "Community Area") %>%
  mutate(CommunityArea = as.factor(CommunityArea))

socio_data <- spatial_count %>% left_join(socio_ind, by = c("CommunityArea" = "Community Area")) %>%
  rename(HardshipIndex = `Hardship Index`, `PovertyRate` = `Poverty Rate`)

socio_train <- socio_data %>% filter(Year < 2019) 
socio_test <- socio_data %>% filter(Year >= 2019)

socio_model <- gam(Count ~ s(Month, bs = "cc") + Year + s(CommunityArea, bs = "mrf", xt = xt) + HardshipIndex, data = socio_train, family = "poisson")

summary(socio_model)
```

# Binomial Classification
In this section we attempt to fit a binary classification model, to predict given certain information whether the crime would lead to an arrest or not. Choosing only to use 2019 data, due to scale of the data set (leaving us with roughly 250,000 observations), as data recorded after is affected strongly by COVID. 

The features we choose to include in this model are:
$$\mathbf{x} = \texttt{(Domestic, Primary Type, Day, Time, Week Day, District})$$

We choose \texttt{Primary Type} over \texttt{FBI Code} to determine the type of crime as during our EDA we found inconsistencies with the classification of \texttt{FBI Code}. With \texttt{Primary Type}, we merge uncommon levels of our factor variable into the level ``OTHER", in which we also include the pre-existing crime type ``OTHER OFFENSE".

For temporal data we include \texttt{Week Day} as a factor variable, encoding date using \texttt{Day} (1-365) and \texttt{Time} as a numeric character between 0-1.

In order to include spatial information, we consider the \texttt{District} variable. We observe that all districts experience reasonable numbers of crimes to be incorporated into analysis, with the exception of District 031. 

While including more information might improve our models, during our EDA we found the inclusion of highly correlated variables to improve the model insignificantly while drastically increasing computational time.

```{r}
crimes <- df %>% 
  filter(year(Date) == 2019, !(District == 031)) %>%
  select(-c(`Community Area`, `Latitude`, `Longitude`)) %>%
  mutate_if(is.character, as.factor) %>%
  mutate(day = yday(Date), 
         wkday = wday(Date), 
         Hour = as.numeric(hour(Date) + minute(Date)/60)/24)

crimes <- crimes %>%
  mutate(`Primary Type` = othering(`Primary Type`, 1000),
         `Primary Type` = fct_collapse(`Primary Type`, "OTHER" = c("OTHER", "OTHER OFFENSE"))) %>%
  mutate() %>%
  droplevels() %>% 
  drop_na()



time.data <- crimes %>% 
  mutate(Hour = as.factor(hour(crimes$Date))) %>% 
  group_by(Hour) %>% 
  dplyr::count(Arrest)
```

Here we plot the proportion of crimes that resulted in arrest for depending on the hour of the day. We can see that the likelihood of an Arrest Resulting from a crime might have dependence on the hour the crime was committed this could be due to the nature of the crimes). However, this seems to indicates that it would be worthwhile to include time as a predictor in our binary classification model.



```{r}
library(ggplot2)
library(cowplot)

arrest_rates <- crimes%>% 
  mutate(Hour = hour(Date)) %>%
  dplyr::group_by(Hour) %>%
  dplyr::summarise(`Total Crimes` = n(), 
                   Arrests = sum(Arrest), 
                   `Arrest Prob` = mean(Arrest))

plot1 <- ggplot(arrest_rates) + 
  geom_point(aes(x =Hour, y = `Arrest Prob`)) + 
  labs(y = 'Proportion of Arrests', x= 'Hour of Day')

plot2 <- ggplot(arrest_rates) +
  geom_point(aes(x =Hour, y = Arrests, color = 'Arrests' )) + 
  geom_point(aes(x =Hour, y = `Total Crimes`, color = 'Total Crimes'))+ 
  labs(y = 'Crimes', x= 'Hour of Day')+ 
  scale_color_manual(values = cbpal[c(3,6)]) +
  theme(legend.position = "bottom", legend.title = element_blank())

plot_grid(plot1, plot2, nrow=2)
```

## Logistic Regression

```{r}
library(caret)
library(doParallel)
library(boot)
library(kernlab)
set.seed(30700) # Setting seed for reproducibility

crimes <- crimes %>% mutate(Arrest = as.factor(Arrest))
```

Using the package `doParallel` and `caret` we can easily fit classification models. We define a control method which allows our function to do 5-fold Cross Validation, using `doParallel` to do the cross validation in parallel to decrease computational time. Setting seeds for our cross-validation function to ensure reproducibility. The function will return the model with the highest accuracy, however we can see the accuracy for the other models in the cross validation process.



```{r message=FALSE, warning=FALSE}
#Create Seeds for the Cross Validation Function
seeds <- vector(mode = "list", length = 6)
for(i in 1:6) seeds[[i]] <- sample.int(n=1000, 3)

#Define Cross Validation Function
control <- trainControl(method = 'cv', number = 5, allowParallel = TRUE, seeds=seeds)

cl <- makePSOCKcluster(detectCores() - 2)

registerDoParallel(cl)

#Fitting Logistic Regression Classifier
log.model <- train(Arrest ~ .,
               data = crimes,
               trControl = control,
               method = "glm",
               family=binomial())

stopCluster(cl)
```


## Support Vector Machine

In this section we fit Support Vector Machines with the Radial Basis Kernel (due to the unclear relationship between our predictors and class).

As SVM models scale badly with data points we have to train our models on samples of our data set (which has greater than $250,000$ data points). We will be fitting 3 SVM models, each using a different sampling technique.

\begin{itemize}
    \item The first method is to take a proportional sample, which attempts to generate a sample where the proportion of each classes emulates the original data set.
    \item The next is known as 'Down Sampling' where we randomly sample the majority class till we have the same number of observations as the minority class.
    \item Similarly we will use the method known as 'Up Sampling', randomly samples (with replacement) the minority class until we have the same number of observations as the majority class.
\end{itemize}

It is clear than in our data there is a significant imbalance in the class proportions, only $21.7%$ of the data has `Arrest = TRUE`. For our first SVM model, we will take a representative sample of roughly 13,000 data points. The `createDataPartition()` function in `caret` allows us to easily do this.


```{r}
trainIndex <- createDataPartition(crimes$Arrest, p = .05, 
                                  list = TRUE)
```


Just like with the Logistic Regression Classifier we use the `train` function from `caret` using our previously defined train function - which has the exact same implementation for the SVMs we fit. The `svmRadial` method uses `kernlab::sigest` to determine a good predictor for $\sigma$. It will then test multiple values for Cost and will choose the one which maximizes accuracy. The `train` function will work in this manner for the other 2 models we train as well.

```{r}
cl <- makePSOCKcluster(detectCores() - 2)

registerDoParallel(cl)

#Fitting SVM Model with a Proportional Sample
svm.model <- train(Arrest ~ .,
                   data = crimesTest,
                   trControl = control,
                   method = 'svmRadial',
                   family = binomial(),
                   allowParallel = TRUE)

stopCluster(cl)
```


As we will be taking a somewhat small sample of our whole data set to train this model on, we will now try a method called down sampling, this a method of sampling a data set in which we take all of the data points from our lowest proportion class, and sample an equal amount of the other proportions of the classes.

First we use the `downSample` function from `caret` this takes a sample of our data set, giving us an equal sample of both `Arrest = TRUE` and `Arrest = FALSE`.

```{r}
down_sample <- downSample(x = crimes, y = crimes$Arrest)
down_sample %>% count(Arrest)
```

We see that the sample involves all the TRUE data points and gives a random sample of our FALSE data points, we can now use the `createDataPartition` function to take a representative sample of `down_sample` (of equal size to our other sample).

```{r}
trainIndexD <- createDataPartition(down_sample$Arrest, p = nrow(crimesTest)/nrow(down_sample), 
                                 list = TRUE)

cl <- makePSOCKcluster(detectCores() - 2)

registerDoParallel(cl)

#Training a SVM using down sampling
svm.modelD <- train(Arrest ~ .,
                   data = crimesTestD,
                   trControl = control,
                   method = 'svmRadial',
                   family = binomial())

stopCluster(cl)
```

There is another method of sampling in a similar vein to down sampling, known as up sampling where we sample with replacement the non majority classes until they are of the same length as the majority class in a similar manner with down sampling we will train a model.

```{r}
up_sample <- upSample(x = crimes, y = crimes$Arrest)
```

```{r}
set.seed(1)
trainIndexU <- createDataPartition(up_sample$Arrest, p = nrow(crimesTest)/nrow(up_sample), 
                                 list = TRUE)

cl <- makePSOCKcluster(detectCores() - 1)

registerDoParallel(cl)

#Training a SVM using up sampling
svm.modelU <- train(Arrest ~ .,
                   data = crimesTestU,
                   trControl = control,
                   method = 'svmRadial',
                   family = binomial())

stopCluster(cl)
svm.modelU$results[c('Accuracy', 'Kappa')]
```

## Model Performance

To test the performance of our Models, we will simulate 10 proportional (in respect to number of arrests), testing data sets. For each of these testing sets we will some metrics. The metrics below will be ones measured for each of models.

$$Accuracy = Pr(\hat{Y} = Y)$$
$$Sensitivity = Pr(\hat{Y} = 1 | Y = 1) = Pr(\text{True Positive})$$
$$Specificity = Pr(\hat{Y} = 0| Y = 0) = Pr(\text{True Negative})$$
$$\text{Balanced Accuracy} = \frac{Sensitivity + Specificity}{2}$$

An interesting benchmark for the usefulness of the model is the No-Information Rate which is the accuracy if we just guess for each data point is the largest proportion class. For the crimes data set that is $0.7828$, however this is not some foolproof benchmark, for example if our goal is to predict to lowest proportion class consistently. 


\textbf{\big REVISIT - Insert code}

We can see that both the proportional SVM Model and Logistic Model performed very similarly, both with high accuracy and specificity, this is to be expected, as both models were trained and tested with far more `FALSE` data points. We can see that with the trade-off being in sensitivity, both performing poorly at distinguishing positive data points. 

Another important point, is that we might have expected the SVM model to perform better than the Logistic Regression Model, due to it's abilities to take into account non-linear relationships; however, we can see they performed very similarly in nearly all aspects, this is probably due to SVM being trained on only $5\%$ of the 2019 data set while our Logistic Model was trained on the full 2019 data set.

As expected up and down sampling had a positive affect on the sensitivity of the model due to the training data involving a larger proportion of `TRUE` values. If we look at the down sampled model, we see that the high sensitivity gives a large trade off with specificity and accuracy (pushing it below the no information rate), this model is far more likely than the other models to give a false positive. This could be due to the down sample involving sampling the `FALSE` data points possibly giving a less informative sample. 

Compared to down sampling, up sampling did not increase the sensitivity as much, but instead found a more balanced trade off with specificity and accuracy - this is interesting given that both models were trained on the same amount of `TRUE` and `FALSE` data points, further analysis would be needed to determine whether this was due to an intrinsic property of the sampling technique or just due to randomness.

The better balanced accuracy of the up and down sampled models, shows that accuracy of the Logistic and proportional models comes heavily from it's lack of sensitivity combined with the imbalanced class proportions, and that depending on what the goal of our models it might be preferable to pick a less overall accurate model.

# Multi-class Classification

```{r, include = FALSE}
# Clear work environment to reset from previous sections
rm(list = ls())
```

In this section we consider the multi-class classification task of predicting the type of a reported crime, given data on the crime. We also extend this to address the secondary question of if we can predict the time of day at which a crime occurRed, given data on the crime. We will focus here on the method of multinomial logistic regression for both of these tasks.

Firstly, we perform feature selection by manual investigation of our variables. We will identify variables to include in our classifier, taking into account both their real world interpretations and computational complexity of the model. We will select several feasible models, and use multiple metrics to assess their performance. We will use repeated $k$-fold cross validation to calculate average values of these metrics across different training sets. We will then compare our models using these averages.
