---
title: "Multiclass Lab Notebook"
author: "Codie Wood"
date: "2023-01-11"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Multi-class Classification

```{r, include = FALSE}
# Clear work environment to reset from previous sections
rm(list = ls())
```

In this section we consider the multi-class classification task of predicting the type of a reported crime, given data on the crime. We also extend this to address the secondary question of if we can predict the time of day at which a crime occurRed, given data on the crime. We will focus here on the method of multinomial logistic regression for both of these tasks.

Firstly, we perform feature selection by manual investigation of our variables. We will identify variables to include in our classifier, taking into account both their real world interpretations and computational complexity of the model. We will select several feasible models, and use multiple metrics to assess their performance. We will use repeated $k$-fold cross validation to calculate average values of these metrics across different training sets. We will then compare our models using these averages.

```{r}
devtools::install_github("codiewood/chicago_crime/chicri")
library(chicri)
```


```{r}
#wont need this when loading package
require(tidyverse) # For data manip
require(lubridate) # For date/time
require(ggplot2) # For plots
require(sf) # For spatial plots
require(caret) # For confusion matrix

theme_set(theme_bw())
```

## Feature Selection

Due to the extremely large size of the full data set, it is computationally intensive to download, store and use to train models. As such, we instead train and test the multinomial logistic regression models only on data from 2019.

```{r}
# data I want here should be a tibble format, with vars primary type, location desc, district, date, arrest, domestic, x coord and y coord. if different, will need to tweak code later on.
dat <- load_crimes_csv("chicri/data/processed/Crimes_2019_Location_Type.csv")
# API stuff below should give same data set
#dat <- load_crimes_API(year="2019")
#vars_to_remove <- c("ID", "Case Number", "Block", "IUCR", "FBI Code", "Description", "Beat", "Ward", "Community Area", "Updated On", "Latitude", "Longitude", "Location", "Year")
#dat <- process_data(dat) %>%
#  select(-all_of(vars_to_remove))
```

As well as the data pre-processing described earlier in the report, we also perform additional feature selection for this particular task. In order to do this we incorporate information from conducting exploratory data analysis, as well as considering computational cost.

### Primary Type

```{r}
# Reorders type factor for plotting
dat$`Primary Type` <- fct_infreq(dat$`Primary Type`) %>% fct_rev()

thr <- 1000 # set threshold for othering
ggplot(dat) +
  geom_bar(aes(y = `Primary Type`)) +
  geom_vline(xintercept=thr, colour="red")
```

We observe that there are initially 31 crime types as classified in our data set. Of these, 14 types have fewer than 1000 observations. In order to reduce the computational complexity of our model, we merge these uncommon levels of our factor variable into the level OTHER, in which we also include the pre-existing crime type OTHER OFFENSE. This leaves us with a total of 17 levels in our variable, with 7.57% of data values in the OTHER category. The least frequent category is SEX OFFENSE, which has 1310 occurences.

```{r}
# Reordering and othering factor levels (for plots and analysis purposes)
dat$`Primary Type` <- fct_recode(dat$`Primary Type`,"OTHER" = "OTHER OFFENSE") %>%
  othering(thr,print_summary = T) %>%
  fct_infreq() %>%
  fct_rev()

ggplot(dat) +
  geom_bar(aes(y = `Primary Type`))
```

### Location Description
BEEP move this to an EDA and just remove the variable earlier?
We also wish to reduce the number of levels for the Location Descriptions in our data set. Initially, we have 156 location descriptions. Of these, 127 have less than 1000 observations. 
In order to reduce the computational complexity of our model, we merge these uncommon levels of our factor variable into the level OTHER
```{r}
# Reordering factor levels (for plots and analysis purposes)
dat$`Location Description` <- fct_infreq(dat$`Location Description`) %>%
  fct_rev()

ggplot(dat) +
  geom_bar(aes(y = `Location Description`)) +
  geom_vline(xintercept=thr, colour="red")
```

```{r}
dat <- regroup_locations(dat,thr)

dat$`Location Description` <- fct_infreq(dat$`Location Description`) %>% fct_rev()

ggplot(dat) +
  geom_bar(aes(y = `Location Description`))
```


### District

```{r}
# Loading mapping data for plots
district_map <- RSocrata::read.socrata("https://data.cityofchicago.org/resource/24zt-jpfn.csv")
```

In order to include spatial information, we consider the District variable. We observe that all districts experience reasonable numbers of crimes to be incorporated into analysis, with the exception of District 031. 

```{r}
district_map <- district_map %>%
  select(c(the_geom, District = dist_num)) %>%
  st_as_sf(wkt = "the_geom") 
plot_heat_map_d(district_map,"District")
```

This district is made up of multiple different areas, as can be seen in the geographical plots. In order to simplify analysis, we exclude the 7 crimes from District 031, and simply note that as a result our model will not be suitable for prediction in this district. As such we are left with 22 districts in our analysis.

```{r}
# Reordering factor levels (for plots and analysis purposes)
dat$District <- dat$District %>%
  fct_infreq() %>%
  fct_rev()

ggplot(dat) +
  geom_bar(aes(y = District))
```

```{r}
# Removing district 31 from analysis
dat <- dat[dat$District != "031",]
dat$District <- fct_drop(dat$District)
```

In order to reduce the computational complexity of our model, whilst still maintaining location information, we can also consider grouping districts further into policing district areas, of which there are 5. 

BEEP Source http://oururbantimes.com/district-stations/command-changes-12th-and-14th-chicago-police-department-districts

https://news.wttw.com/2020/04/30/chicago-police-opening-2-new-operation-areas-expand-resources-across-city

```{r}
# Adding area groupings
area1 <- c("002","003","008","009","007")
area2 <- c("004","005","006","022")
area3 <- c("001","012","018","019","020","024")
area4 <- c("010","011","015")
area5 <- c("014","016","017","025")
dat$`District Area` <- fct_collapse(dat$District, "AREA 1" = area1, "AREA 2" = area2, "AREA 3" = area3, "AREA 4"= area4, "AREA 5" = area5) %>%
  fct_relevel(sort)
```


```{r}
district_map$Area <- fct_collapse(as.factor(district_map$District),
                                  "AREA 1" = as.character(as.integer(area1)),
                                  "AREA 2" = as.character(as.integer(area2)),
                                  "AREA 3" = as.character(as.integer(area3)),
                                  "AREA 4"= as.character(as.integer(area4)),
                                  "AREA 5" = as.character(as.integer(area5))) %>%
  fct_relevel(sort) %>%
  fct_relevel("31", after = Inf)

#Plot for just area
area_plot <- plot_heat_map_d(district_map,"Area")
area_plot
```


```{r}
# Get district labels and locations
district_labels <- sort(as.integer(district_map$District))
district_markers <- district_map %>%
  arrange(as.integer(District)) %>%
  st_centroid() %>%
  st_coordinates() %>%
  as_tibble() %>%
  mutate(district_labels = district_labels)

# Plot with district labels and coloured area: labels for 31 are a bit vom
area_plot_markers <- area_plot +
  geom_sf(data = district_map, aes(fill = Area)) +
    scale_fill_viridis_d(name = "Area",option = "magma") +
  geom_point(data = district_markers, aes(X, Y), size = 7, shape=22, fill = "Grey", alpha=0.7) +
  geom_text(data = district_markers, aes(X, Y, label = district_labels)) +
  theme_void()

area_plot_markers
```

```{r}
# Plot showing number of crimes in each district
ggplot(dat) +
  geom_bar(aes(y = `District Area`))
```

As we are choosing to encode location in our model via district and district area, in order to reduce computational costs and reduce redundancy in our model we will not consider the X and Y coordinates so drop them from our analysis.

```{r}
dat <- select(dat, -c(`X Coordinate`,`Y Coordinate`))
```

### Time and Date

In order to incorporate temporal data, we include the date as the day of the year, `Day`, as well as time of day,`Time`. We standardize both of these variables to take values between 0 and 1. This is to ensure the convergence rate is not slow when we fit our model.

In order to facilitate our second model, which attempts to predict the time of day at which a crime occurs, we also add the variable `Time of Day` which encodes the time as a categorical variable with the following levels:
* `MORNING`: between 5am and 12 noon
* `AFTERNOON`: between 12 noon and 5pm
* `EVENING`: between 5pm and 9pm
* `NIGHT`: between 9pm and 5am

After this processing, we then remove the `Date` variable.

```{r}
t <- lubridate::hour(dat$Date) + lubridate::minute(dat$Date)/60
tod <- rep("NIGHT",length(t))
for(i in 1:length(t)){
  if(5 <= t[i] & t[i] < 12){
    tod[i] <- "MORNING"
    }
  else if(12 <= t[i] & t[i] < 17){
    tod[i] <- "AFTERNOON"
    }
  else if(17 <= t[i] & t[i] < 21){
    tod[i] <- "EVENING"
    }
}
tod <- as.factor(tod)
```

```{r}
dat <- dat %>%
  mutate(dat,
         Day = yday(dat$Date) / 365,
         Time = t/24,
         `Time of Day` = tod) %>%
  select(-Date)
```

```{r}
# Plot showing number of crimes at times of day
ggplot(dat) +
  geom_bar(aes(y = `Time of Day`))
```

## Multinomial Logistic Regression

We use the `multinom()` function from the `nnet` package in order to implement multinomial logistic regression. We calculate various model performance metrics using the `confusionMatrix()` function from the `caret` package. Namely we focus here on calculating overall accuracy and no-information rate (NIR), as well as class specific sensitivity, specification and accuracy values. We implement 5-fold repeated cross validation, with $n = 5$ repeats, and calculate the average of each metric across all folds for each repeat.

BEEP No-information rate is the best case scenario of what would happen if we just assigned all points the one class: we want our model to do better than that.

```{r}
#setting seed for reprodcibility
set.seed(200899)
metrics <- c("Sensitivity", "Specificity", "Balanced Accuracy")
```

### Predicting Crime Type

We begin by splitting our data into our response variable and the variables we want to include in our models. We look at three potential models:
1. No Location: Primary Type ~ Arrest + Domestic + Day + Time
2. District Area: Primary Type ~ Arrest + Domestic + Day + Time + District Area
3. District: Primary Type ~ Arrest + Domestic + Day + Time + District

```{r}
# Splitting response data and different model variable sets
y <- dat$`Primary Type`
X_noloc <- dat %>% select(c(Arrest, Domestic, Day, Time))
X_area <- dat %>% select(c(Arrest, Domestic, Day, Time, `District Area`))
X_district <- dat %>% select(c(Arrest, Domestic, Day, Time, District))
```

We evaluate the model performance using the `mnlr_kfold_cv.df()` function, which outputs a data frame containing the metrics as evaluated for the model fitted on each fold for each repeat. Our package also contains the `mnlr_kfold_cv.list()` function, which performs the same operations and outputs a list based format of our values, as well as their averages. We use the data frame function for our analysis, however, as its output can be easily manipulated to calculate averages, investigate metric distributions. Further, the data frame outputs can be easily converted into an easy to read table, ideal for producing result documentation, which the list output cannot.

```{r, eval=FALSE}
mnlr_results_noloc.df <- mnlr_kfold_cv.df(X = X_noloc, y = y, k = 5, n_reps = 5, metrics = metrics)
```

```{r, eval = FALSE}
mnlr_results_area.df <- mnlr_kfold_cv.df(X = X_area, y = y, k = 5, n_reps = 5, metrics = metrics)
```

```{r, eval=FALSE}
mnlr_results_district.df <- mnlr_kfold_cv.df(X = X_district, y = y, k = 5, n_reps = 5, metrics = metrics)
```

BEEP below loads from files on computer saved from prev run of code so that we can actually knit the pdf, variable names need to stay same BEEP

```{r, include = FALSE}
load("chicri/temp_data/mnlr_results_noloc_df.RData")
load("chicri/temp_data/mnlr_results_area_df.RData")
load("chicri/temp_data/mnlr_results_district_df.RData")
```

BEEP am attempting to write a function that will do this but unsure how to get around the fact that for class based bit, metrics might not be those 3 BEEP
```{r}
noloc_overall_means <- mean_repeat_metrics(mnlr_results_noloc.df, type = "overall")
noloc_overall_means

noloc_class_means <- mnlr_results_noloc.df$byclass %>%
  dplyr::group_by(Class,Repeat) %>%
  dplyr::summarise("Average Sensitivity" = mean(Sensitivity),
                   "Average Specificity" = mean(Specificity),
                   "Average Balanced Accuracy" = mean(`Balanced Accuracy`)) %>%
  dplyr::relocate(Repeat) %>%
  dplyr::arrange(Repeat)

```
BEEP note that the overall means all look the same because of rounding, they aren't actually all equal BEEP

```{r}
noloc_avg_class_means <- noloc_class_means %>%
  dplyr::summarise("Average Sensitivity" = mean(`Average Sensitivity`),
                   "Average Specificity" = mean(`Average Specificity`),
                   "Average Balanced Accuracy" = mean(`Average Balanced Accuracy`))
noloc_avg_class_means
```


We use the `knitr::kable()` functionality, as well as the `kableExtra` package, to export our results into a format suitable for our report.

```{r}
noloc_kab <- knitr::kable(noloc_overall_means,
                          format="latex",
                          digits = 4,
                          align = rep("c",4)) %>%
  kableExtra::kable_styling(latex_options = "hold_position")
noloc_kab
```

```{r}
noloc_kab_class <- knitr::kable(noloc_avg_class_means,
                          format="latex",
                          digits = 4,
                          align = c("l",rep("c",3))) %>%
  kableExtra::kable_styling(latex_options = "hold_position")
noloc_kab_class
```

```{r}
area_class_means <- mnlr_results_area.df$byclass %>%
  dplyr::group_by(Class,Repeat) %>%
  dplyr::summarise("Average Sensitivity" = mean(Sensitivity),
                   "Average Specificity" = mean(Specificity),
                   "Average Balanced Accuracy" = mean(`Balanced Accuracy`)) %>%
  dplyr::relocate(Repeat) %>%
  dplyr::arrange(Repeat)

area_class_means

area_overall_means <- mean_repeat_metrics(mnlr_results_area.df, type = "overall")
```

```{r}
area_avg_class_means <- area_class_means %>%
  dplyr::summarise("Average Sensitivity" = mean(`Average Sensitivity`),
                   "Average Specificity" = mean(`Average Specificity`),
                   "Average Balanced Accuracy" = mean(`Average Balanced Accuracy`))
area_avg_class_means
```

```{r}
area_kab <- knitr::kable(area_overall_means,
                          format="latex",
                          digits = 4,
                          align = rep("c",4)) %>%
  kableExtra::kable_styling(latex_options = "hold_position")
area_kab
```

```{r}
area_kab_class <- knitr::kable(area_avg_class_means,
                          format="latex",
                          digits = 4,
                          align = c("l",rep("c",3))) %>%
  kableExtra::kable_styling(latex_options = "hold_position")
area_kab_class
```


```{r}
district_class_means <- mnlr_results_district.df$byclass %>%
  dplyr::group_by(Class,Repeat) %>%
  dplyr::summarise("Average Sensitivity" = mean(Sensitivity),
                   "Average Specificity" = mean(Specificity),
                   "Average Balanced Accuracy" = mean(`Balanced Accuracy`)) %>%
  dplyr::relocate(Repeat) %>%
  dplyr::arrange(Repeat)

district_class_means

district_overall_means <- mean_repeat_metrics(mnlr_results_district.df, type = "overall")
```

```{r}
district_avg_class_means <- district_class_means %>%
  dplyr::summarise("Average Sensitivity" = mean(`Average Sensitivity`),
                   "Average Specificity" = mean(`Average Specificity`),
                   "Average Balanced Accuracy" = mean(`Average Balanced Accuracy`))
district_avg_class_means
```


```{r}
district_kab <- knitr::kable(district_overall_means,
                          format="latex",
                          digits = 4,
                          align = rep("c",4)) %>%
  kableExtra::kable_styling(latex_options = "hold_position")
district_kab
```

```{r}
district_kab_class <- knitr::kable(district_avg_class_means,
                          format="latex",
                          digits = 4,
                          align = c("l",rep("c",3))) %>%
  kableExtra::kable_styling(latex_options = "hold_position")
district_kab_class
```

BEEP think best model is using area because not much difference going to district and also has the advantage of lesser computational complexity so easier to run and interpret. Probably explain this in latex report not here? BEEP


```{r, eval=FALSE}
set.seed(1)
ind <- sample(2,nrow(dat),replace=TRUE, prob = c(0.8,0.2))
index <- which(ind == 2)
mnlr <- mnlr_cv_indexed(X_area,y,index,return_model = TRUE)
```
BEEP load for knitting purposes BEEP
```{r, include=FALSE}
load("chicri/temp_data/mnlr_area_model.RData")
```
We look at a subset of the predictive probabilities produced by the model, as well as the coefficient values, in order to interpret the relationships between our observed variables and the log odds of a particular class.
```{r}
head(pp <- fitted(mnlr$model)) #Predictive probabilities
```

```{r}
coef(mnlr$model)
```

```{r}
# Extract confusion matrix for plot
tab <- mnlr$conf.matrix$table
confusion_df <- as.data.frame(tab)

confusion_plot <- ggplot(confusion_df, aes(x=Reference, y = Prediction, fill=Freq)) +
  coord_equal() +
  geom_tile() +
  labs(title = "Prediction distribution") +
  scale_y_discrete(limits = rev) +
  scale_x_discrete(expand = expansion(mult = c(0,0)), guide = guide_axis(angle = 45)) +
  geom_text(aes(label=Freq), color="black") # printing values
confusion_plot
```
In the confusion matrix, we observe that the model only makes predictions for the categories "NARCOTICS", "BATTERY" and "THEFT". We note that, from earlier exploratory analysis, "BATTERY" and "THEFT" are the two most common crimes, however "NARCOTICS" is a less common crime. BEEP this is interesting but idk what to say about it if u have any ideas feel free BEEP

## Predicting Time of Crime

As an extension to this multiclass classification task, we will also evaluate the performance of another multinomial regression classifier used to predict the time of day at which a crime occurred. We use the same methods as seen previously but now time of day is response variable and we exclude Time from our model. This gives us the three models:
1. No Location: Time of Day ~ Primary Type + Arrest + Domestic + Day
2. District: Time of Day ~ Primary Type + Arrest + Domestic + Day + District
3. District Area: Time of Day ~ Primary Type + Arrest + Domestic + Day + District Area

```{r}
#setting seed for reproducibility
set.seed(200899)
# Splitting response data and different model variable sets
y_tod <- dat$`Time of Day`
X_tod_noloc <- dat %>% select(c(`Primary Type`,Arrest,Domestic,Day))
X_tod_area <- dat %>% select(c(`Primary Type`,Arrest,Domestic,Day,`District Area`))
X_tod_district <- dat %>% select(c(`Primary Type`,Arrest,Domestic,Day,District))
```

BEEP this has 60 variables and converges (but other models none converged) BEEP
```{r, eval=FALSE}
tod_results_noloc <- mnlr_kfold_cv.df(X = X_tod_noloc, y = y_tod, k = 5, n_reps = 5, metrics = metrics)
```

BEEP this has 72 variables and converges BEEP
```{r, eval = FALSE}
tod_results_area <- mnlr_kfold_cv.df(X = X_tod_area, y = y_tod, k = 5, n_reps = 5, metrics = metrics)
```

BEEP this has 123 variables BEEP
```{r, eval=FALSE}
tod_results_district <- mnlr_kfold_cv.df(X = X_tod_district, y = y_tod, k = 5, n_reps = 5, metrics = metrics)
```
BEEP need to run the above and save then find best and do plots as before BEEP

```{r, include = FALSE}
load("chicri/temp_data/tod_results_noloc.RData")
load("chicri/temp_data/tod_results_area.RData")
load("chicri/temp_data/tod_results_district.RData")
```

```{r}
tod_noloc_class_means <- tod_results_noloc$byclass %>%
  dplyr::group_by(Class,Repeat) %>%
  dplyr::summarise("Average Sensitivity" = mean(Sensitivity),
                   "Average Specificity" = mean(Specificity),
                   "Average Balanced Accuracy" = mean(`Balanced Accuracy`)) %>%
  dplyr::relocate(Repeat) %>%
  dplyr::arrange(Repeat)

tod_noloc_class_means

tod_noloc_overall_means <- mean_repeat_metrics(tod_results_noloc,type="overall")

tod_noloc_overall_means
```

```{r}
tod_noloc_kab <- knitr::kable(tod_noloc_overall_means,
                          format="latex",
                          digits = 4,
                          align = rep("c",4)) %>%
  kableExtra::kable_styling(latex_options = "hold_position")
tod_noloc_kab
```

```{r}
tod_area_class_means <- tod_results_area$byclass %>%
  dplyr::group_by(Class,Repeat) %>%
  dplyr::summarise("Average Sensitivity" = mean(Sensitivity),
                   "Average Specificity" = mean(Specificity),
                   "Average Balanced Accuracy" = mean(`Balanced Accuracy`)) %>%
  dplyr::relocate(Repeat) %>%
  dplyr::arrange(Repeat)

tod_area_class_means

tod_area_overall_means <- mean_repeat_metrics(tod_results_area,type="overall")

tod_area_overall_means
```

```{r}
tod_area_kab <- knitr::kable(tod_area_overall_means,
                          format="latex",
                          digits = 4,
                          align = rep("c",4)) %>%
  kableExtra::kable_styling(latex_options = "hold_position")
tod_area_kab
```


```{r}
tod_district_class_means <- tod_results_district$byclass %>%
  dplyr::group_by(Class,Repeat) %>%
  dplyr::summarise("Average Sensitivity" = mean(Sensitivity),
                   "Average Specificity" = mean(Specificity),
                   "Average Balanced Accuracy" = mean(`Balanced Accuracy`)) %>%
  dplyr::relocate(Repeat) %>%
  dplyr::arrange(Repeat)

tod_district_class_means

tod_district_overall_means <- mean_repeat_metrics(tod_results_district,type="overall")

tod_district_overall_means
```

```{r}
tod_district_kab <- knitr::kable(tod_district_overall_means,
                          format="latex",
                          digits = 4,
                          align = rep("c",4)) %>%
  kableExtra::kable_styling(latex_options = "hold_position")
tod_district_kab
```
Each of the models have similar overall accuracy across our repeats, so we prefer the simplest model with the fewest variables as this has minimal computational cost to run. BEEP this is super arbitrary BEEP
```{r, eval=FALSE}
set.seed(1)
ind <- sample(2,nrow(dat),replace=TRUE, prob = c(0.8,0.2))
index <- which(ind == 2)
tod_mnlr <- mnlr_cv_indexed(X_tod_noloc,y_tod,index,return_model = TRUE)
```
BEEP load for knitting purposes BEEP
```{r, include=FALSE}
load("chicri/temp_data/tod_noloc_model.RData")
```

```{r}
head(tod_pp <- fitted(tod_mnlr$model)) #Predictive probabilities for each obs
```

```{r}
coef(tod_mnlr$model)
```

```{r}
# Extract confusion matrix for plot
tod_tab <- tod_mnlr$conf.matrix$table
tod_confusion_df <- as.data.frame(tod_tab)

tod_confusion_plot <- ggplot(tod_confusion_df, aes(x=Reference, y = Prediction, fill=Freq)) +
  coord_equal() +
  geom_tile() +
  labs(title = "Prediction distribution") +
  scale_y_discrete(limits = rev) +
  scale_x_discrete(expand = expansion(mult = c(0,0)), guide = guide_axis(angle = 45)) +
  geom_text(aes(label=Freq), color="black") # printing values
tod_confusion_plot
```

