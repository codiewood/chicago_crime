---
title: "project"
output:
  pdf_document: default
  html_document: default
date: "2023-01-09"
---
```{r}
rm(list = ls())
```

```{r warning=FALSE}
library(plyr)
library(tidyr)
detach(package:plyr) 
library(dplyr)
library(lubridate)
library(forcats)
library(stringr)
library(readr)
library(boot)
library(caret)
library(kernlab)

data_all <- read_csv('Crimes_2019.csv',show_col_types = FALSE)



data <- data_all %>% select(-c(ID, `Case Number`, Block, IUCR, Description, Beat, Ward, `Community Area`, `Updated On`, Latitude, Longitude, Location, Year, `Location Description`))

data$Date <- parse_datetime(data$Date, format = "%m/%d/%Y %I:%M:%S %p")

```

```{r}
othering <- function(factor_vec, threshold){
  level <- levels(factor_vec) 
  tab <- tabulate(factor_vec)
  other.levels <- level[ tab < threshold]
  factor_vec <- fct_collapse(factor_vec, "OTHER" = other.levels)
  perc <- length(factor_vec[factor_vec == 'OTHER'])*100/length(factor_vec)
  print(perc)
  return(factor_vec)
  
}
```


```{r}
crimes <- drop_na(data_all)

crimes$Date <- parse_datetime(crimes$Date, format = "%m/%d/%Y %I:%M:%S %p")

crimes <- crimes %>% select(-c(ID, `Case Number`, Block, IUCR, Description, Beat, Ward, `Community Area`, `Updated On`, Latitude, Longitude, Location, Year, 'FBI Code', 'Location Description'))

crimes[c('Primary Type', 'District', 'Arrest', 'Domestic')] <- lapply(crimes[c('Primary Type', 'District', 'Arrest', 'Domestic')],as.factor)

time.data <- crimes %>% mutate(Hour = as.factor(hour(crimes$Date))) %>% group_by(Hour) %>% dplyr::count(Arrest)

crimes <- crimes %>% mutate(day = yday(crimes$Date), wkday = wday(crimes$Date), Hour = as.numeric(hour(crimes$Date) + minute(crimes$Date)/60)) %>% select(-c(Date,'X Coordinate','Y Coordinate'))

crimes[c('Primary Type', 'District', 'Arrest', 'Domestic','wkday')] <- lapply(crimes[c('Primary Type', 'District', 'Arrest', 'Domestic','wkday')],as.factor)

crimes$District[crimes$District == '031'] <- NA

crimes<- drop_na(crimes)

crimes<- droplevels(crimes)

levels(crimes$`Primary Type`)[levels(crimes$`Primary Type`)=="OTHER OFFENSE"] <- "OTHER"

crimes$`Primary Type` <- othering(crimes$`Primary Type`, 1000)

crimes<- droplevels(crimes)
```
For some EDA on the importance of including time data in our model for classifying likelihood of arrest, we plot the probability of arrest for each hour (using 2019 data), we see that the proportion of arrests are inconsistent depending on time - this could be due to types of crimes expected at each time. This makes it seem significant to at least include in the model. BEEP

```{r}
#calculate proportion of crimes resulted in arrest in each hour
proparr <- c()
arr <- c()
tot <- c()
for (i in 1:(nrow(time.data)/2) ){
  prop <- time.data$n[2*i]/(time.data$n[2*i-1] + time.data$n[2*i]) 
  proparr <- c(proparr,prop)
  arr <-c(arr, time.data$n[2*i])
  tot <- c(tot,time.data$n[2*i-1] + time.data$n[2*i])
}

```

```{r}
propdf <- data.frame(cbind(0:23,proparr))
arrdf <- data.frame(cbind(0:23,arr, tot))
colnames(arrdf) <- c('Hour', 'Arr', 'Total')
colnames(propdf) <- c('Hour', 'ArrProp')
```

```{r warning=FALSE}
library(ggplot2)
library(cowplot)
plot1 <- ggplot(propdf) + geom_point(aes(x =Hour, y = ArrProp)) + labs(y = 'Proportion of Arrests', x= 'Hour of Day')

plot2 <- ggplot(arrdf) + geom_point(aes(x =Hour, y = Arr, color = 'Arrests' )) + geom_point(aes(x =Hour, y = Total, color = 'Total Crimes'))+ labs(y = 'Crimes', x= 'Hour of Day')+ theme(legend.position = "bottom")

plot_grid(plot1, plot2, nrow=2)
```

Here I use the Caret Library to train some SVM models and a Logistic Regression Model.

```{r}
library(e1071)  
library(caret)
```

We will attempt to fit our SVM using 2 different training sets. As SVM models scale badly with data points we have to train our models on samples of our data set (which has greater than $250,000$ data points). There are a few methodologies when taking these samples, we could randomly sample our data, we could also take a representative sample and we could also try having equal amount of samples from each of the classes.

```{r}
crimes %>% dplyr::count(Arrest)
```

We see that only $21.7%$ of the data has `Arrest = TRUE`. For our first SVM model, we just take a representative sample (of the same size as the previous model). The `createDataPartition` function in `caret` allows us to easily do this.


```{r}
set.seed(1) # Setting seed for reproducibility
trainIndex <- createDataPartition(crimes$Arrest, p = .05, 
                                  list = TRUE)

crimesTest <- crimes[trainIndex$Resample,]

crimesTest %>% dplyr::count(Arrest)
```

Using the `train` function from `caret` we can train our classifier using SVM with a Radial Basis Kernel, I use the package `doParallel` to allow us to do $5$ fold cross validation in parallel.

```{r}
library(doParallel)

#create a list of seeds, and change for each resampling
#length is = (n_repeats*nresampling)+1

seeds <- vector(mode = "list", length = 6)

for(i in 1:5) seeds[[i]] <- sample.int(n=1000, 3)

#for the last model
seeds[[6]]<-sample.int(1000, 1)
# we will reuse these seeds for different models

#Define function to Cross-Validation in Parallel
control <- trainControl(method = 'cv',5, allowParallel = TRUE, seeds=seeds)

cl <- makePSOCKcluster(detectCores() - 1)

registerDoParallel(cl)

svm.model <- train(Arrest ~ .,
                   data = crimesTest,
                   trControl = control,
                   method = 'svmRadial',
                   family = binomial(),
                   allowParallel = TRUE)

stopCluster(cl)
svm.model$results[c('Accuracy', 'Kappa')]
```


As we will be taking a somewhat small sample of our whole data set to train this model on, we can try a method called down sampling, this a method of sampling a data set in which we take all of the data points from our lowest proportion class, and sample an equal amount of the other proportions of the classes.

First we use the `downSample` function from `caret` this takes a sample of our data set, giving us an equal sample of both `Arrest = TRUE` and `Arrest = FALSE`.

```{r}
down_sample <- downSample(x = crimes, y = crimes$Arrest)
down_sample %>% count(Arrest)
```
We see that the sample involves all the TRUE data points and gives a random sample of our FALSE data points, we can now use the `createDataPartition` function to take a representative sample of `down_sample` (of equal size to our other sample).

```{r}
set.seed(1)
trainIndexD <- createDataPartition(down_sample$Arrest, p = nrow(crimesTest)/nrow(down_sample), 
                                 list = TRUE)

crimesTestD <- down_sample[trainIndexD$Resample,] %>% select(-Class)

crimesTestD %>% count(Arrest)
```

```{r}
cl <- makePSOCKcluster(detectCores() - 1)

registerDoParallel(cl)

svm.modelD <- train(Arrest ~ .,
                   data = crimesTestD,
                   trControl = control,
                   method = 'svmRadial',
                   family = binomial())

stopCluster(cl)
svm.modelD$results[c('Accuracy', 'Kappa')]
```

There is another method of sampling in a similar vein to down sampling, known as up sampling where we sample with replacement the non majority classes until they are of the same length as the majority class. 

```{r}
up_sample <- upSample(x = crimes, y = crimes$Arrest)
up_sample %>% count(Arrest)
```


```{r}
set.seed(1)
trainIndexU <- createDataPartition(up_sample$Arrest, p = nrow(crimesTest)/nrow(up_sample), 
                                 list = TRUE)

crimesTestU <- up_sample[trainIndexU$Resample,] %>% select(-Class)

crimesTestU %>% count(Arrest)
```



```{r}
cl <- makePSOCKcluster(detectCores() - 1)

registerDoParallel(cl)

svm.modelU <- train(Arrest ~ .,
                   data = crimesTestU,
                   trControl = control,
                   method = 'svmRadial',
                   family = binomial())

stopCluster(cl)
svm.modelU$results[c('Accuracy', 'Kappa')]
```

We can similarly train a logistic regression model using 5-fold cross validation.

```{r}
cl <- makePSOCKcluster(detectCores() - 1)

registerDoParallel(cl)

log.model <- train(Arrest ~ .,
               data = crimes,
               trControl = control,
               method = "glm",
               family=binomial())

stopCluster(cl)
log.model$results[c('Accuracy', 'Kappa')]
```

There are many different ways to measure 
$$Accuracy = Pr(\hat{Y} = Y)$$
$$Sensitivity = Pr(\hat{Y} = 1 | Y = 1) = Pr(\text{True Positive})$$
$$Specificity = Pr(\hat{Y} = 0| Y = 0) = Pr(\text{True Negative})$$
$$\text{Balanced Accuracy} = \frac{Sensitivity + Specificity}{2}$$

An interesting benchmark for the usefulness of the model is the No-Information Rate which is the accuracy if we just guess for each data point is the largest percentage data point. For the crimes dataset that is $0.7828$. 

```{r echo=FALSE}
#confusionMatrix(reference = crimes$Arrest, data = predict(svm.model, newdata = crimes), positive = 'TRUE')

#confusionMatrix(reference = crimes$Arrest, data = predict(svm.modelD, newdata = crimes), positive = 'TRUE')

confusionMatrix(reference = crimes$Arrest, data = predict(log.model), positive = 'TRUE') ## look into plotting side by side
```

```{r}
blank <- data.frame(ACC=numeric(0),SENS = numeric(0), SPEC = numeric(0), BALACC = numeric(0)) 
up <- blank
down <- blank
logistic <- blank 
repSVM <- blank

for (testrow in 1:10){
  sampIndex <- createDataPartition(crimes$Arrest, p = .025, 
                                  list = TRUE)
  
  sampTest <- crimes[sampIndex$Resample,]
  
  u <- confusionMatrix(data = predict(svm.modelU, newdata = sampTest), reference = sampTest$Arrest, positive = 'TRUE')
  d <- confusionMatrix(data = predict(svm.modelD, newdata = sampTest), reference = sampTest$Arrest, positive = 'TRUE')
  l <- confusionMatrix(data = predict(log.model, newdata = sampTest), reference = sampTest$Arrest, positive = 'TRUE')
  r <- confusionMatrix(data = predict(svm.model, newdata = sampTest), reference = sampTest$Arrest, positive = 'TRUE')
  
  up[testrow, ] <- c(u[["overall"]][["Accuracy"]], u[["byClass"]][["Sensitivity"]],
               u[["byClass"]][["Specificity"]], u[["byClass"]][["Balanced Accuracy"]])
  
  down[testrow, ] <- c(d[["overall"]][["Accuracy"]], d[["byClass"]][["Sensitivity"]],
               d[["byClass"]][["Specificity"]], d[["byClass"]][["Balanced Accuracy"]])
  
  logistic[testrow, ] <- c(l[["overall"]][["Accuracy"]], l[["byClass"]][["Sensitivity"]],
               l[["byClass"]][["Specificity"]], l[["byClass"]][["Balanced Accuracy"]])
  
  repSVM[testrow, ] <- c(r[["overall"]][["Accuracy"]], r[["byClass"]][["Sensitivity"]],
               r[["byClass"]][["Specificity"]], r[["byClass"]][["Balanced Accuracy"]])
}
```

```{r}
ggplot(mapping = aes(ACC,SENS, color = Model)) + 
  geom_point(data = repSVM, mapping = aes(x = ACC, y = SENS, color = 'SVM')) +
  geom_point(data = down, mapping = aes(x = ACC, y = SENS, color = 'Down SVM')) +
  geom_point(data = logistic, mapping = aes(x = ACC, y = SENS, color = 'Logistic Regression')) + 
  geom_point(data = up, mapping = aes(x = ACC, y = SENS, color = 'Up SVM')) + 
  labs(title = 'Accuracy vs Sensitivity', x = 'Accuracy', y = 'Sensitivity') +
  geom_vline(mapping = aes(xintercept = 0.7828, colour = 'No Information Rate'), size = 1)
```

```{r}
ggplot(mapping = aes(ACC,SPEC, color = Model)) + 
  geom_point(data = repSVM, mapping = aes(x = ACC, y = SPEC, color = 'SVM')) +
  geom_point(data = down, mapping = aes(x = ACC, y = SPEC, color = 'Down SVM')) +
  geom_point(data = logistic, mapping = aes(x = ACC, y = SPEC, color = 'Logistic Regression')) + 
  geom_point(data = up, mapping = aes(x = ACC, y = SPEC, color = 'Up SVM')) + 
  labs(title = 'Accuracy vs Specificity', x = 'Accuracy', y = 'Specificity') 
```
```{r}
ggplot(mapping = aes(ACC,BALACC, color = Model)) + 
  geom_point(data = repSVM, mapping = aes(x = ACC, y = BALACC, color = 'SVM')) +
  geom_point(data = down, mapping = aes(x = ACC, y = BALACC, color = 'Down SVM')) +
  geom_point(data = logistic, mapping = aes(x = ACC, y = BALACC, color = 'Logistic Regression')) + 
  geom_point(data = up, mapping = aes(x = ACC, y = BALACC, color = 'Up SVM')) + 
  labs(title = 'Accuracy vs Balanced Accuracy', x = 'Accuracy', y = 'Balanced Accuracy')
```


```{r}
ModelQual <- cbind(Model = as.character(), blank)
ModelQual[1,] <- c('LogReg', logistic %>% summarise(across(everything(), mean)))
ModelQual[2,] <- c('DownSVM', down %>% summarise(across(everything(), mean)))
ModelQual[3,] <- c('UpSVM', up %>% summarise(across(everything(), mean)))
ModelQual[4,] <- c('SVM', repSVM %>% summarise(across(everything(), mean)))
ModelQual
```


